

# MLOps Dashboard ‚Äî YouTube Gaming Trend Analysis & Model Monitoring

##  Overview

This project implements an **end-to-end MLOps pipeline** for analyzing **YouTube gaming trends** and monitoring **machine learning inference performance** in production.

The system is designed to identify:

* **Games and genres that are currently popular or trending**
* **Live prediction distributions from deployed ML models**
* **Inference performance and reliability metrics** (latency, throughput, errors)

The insights generated by this system are particularly useful for:

* **Content creators and streamers** who want to publish videos about games that are currently popular or likely to trend
* **Data scientists and ML engineers** who want to demonstrate a production-ready MLOps workflow
* **Academic or portfolio purposes** to showcase applied ML monitoring and analytics

---

## System Architecture

The project is composed of several Dockerized services orchestrated using **Docker Compose**:

* **Scraper Service**
  Collects YouTube gaming data and stores it in MongoDB.

* **MongoDB**
  Stores raw and enriched video metadata (views, channels, genres, games).

* **Model Training (Offline)**
  Machine learning models are trained and tuned separately, producing serialized model artifacts.

* **Inference API (FastAPI)**
  Loads the trained model and exposes prediction endpoints.

* **Traffic Generator**
  Simulates user traffic to the inference API for monitoring and testing.

* **Prometheus**
  Collects inference metrics (RPS, latency, errors, predictions).

* **Grafana**
  Visualizes monitoring metrics and alerts.

* **Streamlit Dashboard**
  Displays analytics, trends, and monitoring summaries in a single interface.

---

## How to Run the Project

### 1. Prerequisites

* Docker & Docker Compose
* Git
* (Optional) Python 3.9+ for local experimentation

---

### 2. Clone the Repository

```bash
git clone https://github.com/your-username/project_full_Aldy-Naufal.git
cd project_full_Aldy-Naufal
```

---

### 3. Environment Variables

Create a `.env` file if needed (example):

```env
MONGO_URI=mongodb://mongo:27017
MONGO_DB=game_mlop
PROMETHEUS_URL=http://prometheus:9090
```

---

### 4. Build and Run Services

```bash
docker compose up -d --build
```

This will start:

* MongoDB
* Scraper
* Inference API
* Traffic Generator
* Prometheus
* Grafana
* Streamlit Dashboard

---

## üåê Service Endpoints

| Service             | URL                                                            |
| ------------------- | -------------------------------------------------------------- |
| Inference API       | [http://localhost:8000](http://localhost:8000)                 |
| Health Check        | [http://localhost:8000/health](http://localhost:8000/health)   |
| Predict Endpoint    | [http://localhost:8000/predict](http://localhost:8000/predict) |
| Prometheus          | [http://localhost:9090](http://localhost:9090)                 |
| Grafana             | [http://localhost:3300](http://localhost:3300)                 |
| Streamlit Dashboard | [http://localhost:8501](http://localhost:8501)                 |

---

## API Routes

### `POST /predict`

Performs genre prediction based on input data.

Example request:

```json
{
  "title": "Epic Free Fire Ranked Match",
  "description": "Insane clutch moments",
  "tags": "free fire battle royale"
}
```

Example response:

```json
{
  "predicted_genre": "Battle Royale",
  "model_version": "1.0"
}
```

---

### `GET /metrics`

Exposes Prometheus-compatible metrics, including:

* Request rate
* Error rate
* Latency histogram
* Prediction distribution per genre

---

## Dashboard Features

### Streamlit Dashboard

The Streamlit application provides:

* **Top YouTube channels by total views**
* **Genres ranked by total views (trend indicator)**
* **Top games by number of videos**
* **Live prediction distribution from the inference model**
* **Inference monitoring summary (from Prometheus)**

All information is presented on **a single-page layout** for clarity and reporting purposes.

---

## Monitoring & Alerting

### Prometheus

Prometheus continuously collects:

* Requests per second (RPS)
* Error rates
* Latency (p95)
* In-progress inference requests
* Prediction counts per genre

### Grafana

Grafana is used to:

* Visualize inference performance
* Define alert rules (e.g., high latency, high error rate)
* Serve as the primary monitoring dashboard

Alerts configured in Grafana are also **queried and displayed in Streamlit**, removing the need for duplicate error charts.

---

## CI/CD Pipeline (GitHub Actions)

This project applies **CI/CD practices for MLOps** using **GitHub Actions**.

### Pipeline Trigger

The CI/CD pipeline is automatically triggered when:

* A new model artifact is pushed
* The inference or pipeline code is updated

---

### CI/CD Workflow Stages

The pipeline includes the following stages:

1. **Source Control Trigger**

   * Activated on `git push`

2. **Environment Setup**

   * Python environment initialization
   * Dependency installation

3. **Model Verification**

   * Ensures the latest model artifact is available
   * Verifies the model can be loaded correctly

4. **Build Validation**

   * Docker image build
   * Dependency and configuration checks

5. **Deployment Readiness**

   * Model is confirmed ready for inference
   * Monitoring stack can immediately track the new version

The full workflow configuration and execution history can be found in the **GitHub Actions** tab of this repository.

---

### Benefits of CI/CD in This Project

* Prevents unverified models from being deployed
* Improves reproducibility and reliability
* Enables rapid iteration and experimentation
* Aligns the project with real-world MLOps best practices

---

## Use Case Summary

This system enables:

* **Trend analysis** of games that are widely played or gaining popularity
* **Strategic content planning** for YouTube streamers and gaming creators
* **Production-grade ML monitoring** with live metrics and alerts
* **Academic and professional demonstration** of a complete MLOps lifecycle

---

## Notes

* Model training is performed offline; only validated models are deployed.
* Traffic generator is used to simulate realistic inference loads.
* The dashboard is intentionally designed without auto-refresh to support rebuild-based updates.
